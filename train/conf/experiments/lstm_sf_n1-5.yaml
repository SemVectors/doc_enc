# @package _global_
defaults:
  - /base_config
  - override /trainer: full-sents 
  - override /trainer/optim: adam_with_one_cycle
  - override /model/sent/encoder: lstm
  - override /model/fragment: null
  - override /model/doc: lstm
  - _self_

text_proc:
  tokenizer:
    # add_bos: true
    vocab_path: /train/bpe_256k_from_600M.spm.model
    enable_sampling: true
    alpha: 0.05

trainer:
  print_gpu_memory_stat_every: 1000
  tasks:
    - SENT_RETR
  eval_tasks:
    - SENT_RETR


  optim:
    weight_decay: 0.1
    optim_kind: ADAMW
    use_zero_optim: false
    emb:
      lr: 0.007
      max_grad_norm: 1.0
    sent:
      lr: 0.003
      max_grad_norm: 3.0


model:
  max_chunk_size: 2048
  scale: 20
  margin: 0.0
  sent:
    max_chunk_size: 2048
    scale: 60
    margin: 0.3

batches:
  sents_batch_iterator_conf:
    async_generators: 3
    batch_generator_conf:
      max_sent_size: 128
      batch_size: 2048
      dont_use_hns: true

